---
title: DirectML 簡介
description: 直接機器學習 (DirectML) 是適用于機器學習 (ML) 的低層級 API。
ms.custom: Windows 10 May 2019 Update
ms.localizationpriority: high
ms.topic: article
ms.date: 04/19/2019
ms.openlocfilehash: 2dd37bc4c27364e26e4bbd4ae2cf5d43031c3314
ms.sourcegitcommit: 2d531328b6ed82d4ad971a45a5131b430c5866f7
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 09/16/2019
ms.locfileid: "74103786"
---
# <a name="introduction-to-directml"></a><span data-ttu-id="5ba3a-103">DirectML 簡介</span><span class="sxs-lookup"><span data-stu-id="5ba3a-103">Introduction to DirectML</span></span>

## <a name="summary"></a><span data-ttu-id="5ba3a-104">總結</span><span class="sxs-lookup"><span data-stu-id="5ba3a-104">Summary</span></span>

<span data-ttu-id="5ba3a-105">直接機器學習 (DirectML) 是適用于機器學習 (ML) 的低層級 API。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-105">Direct Machine Learning (DirectML) is a low-level API for machine learning (ML).</span></span> <span data-ttu-id="5ba3a-106">硬體加速機器學習服務基本 (稱為運算子) 是 DirectML 的構成要素。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-106">Hardware-accelerated machine learning primitives (called operators) are the building blocks of DirectML.</span></span> <span data-ttu-id="5ba3a-107">您可以從這些構成要素開發像是消除倍增、消除鋸齒和樣式轉移等機器學習技術，以提供更多的名稱。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-107">From those building blocks, you can develop such machine learning techniques as upscaling, anti-aliasing, and style transfer, to name but a few.</span></span> <span data-ttu-id="5ba3a-108">例如，Denoising 和超解析度，可讓您以較少的每圖元光線來達成令人印象深刻的 raytraced 效果。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-108">Denoising and super-resolution, for example, allow you to achieve impressive raytraced effects with fewer rays per pixel.</span></span>

<span data-ttu-id="5ba3a-109">您可以將機器學習推斷工作負載整合到您的遊戲、引擎、中介軟體、後端或其他應用程式中。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-109">You can integrate machine learning inferencing workloads into your game, engine, middleware, backend, or other application.</span></span> <span data-ttu-id="5ba3a-110">DirectML 具有熟悉的 (原生 c + +、nano COM) DirectX 12 樣式的程式設計介面和工作流程，並受到所有 DirectX 12 相容硬體的支援。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-110">DirectML has a familiar (native C++, nano-COM) DirectX 12-style programming interface and workflow, and it's supported by all DirectX 12-compatible hardware.</span></span> <span data-ttu-id="5ba3a-111">針對 DirectML 範例應用程式（包括基本 DirectML 應用程式的範例），請參閱 [DirectML 範例應用](dml-min-app.md)程式。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-111">For DirectML sample applications, including a sample of a minimal DirectML application, see [DirectML sample applications](dml-min-app.md).</span></span>

<span data-ttu-id="5ba3a-112">DirectML 是在 Windows 10 1903 版和對應版本的 Windows SDK 中引進。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-112">DirectML is introduced in Windows 10, version 1903, and in the corresponding version of the Windows SDK.</span></span>

## <a name="is-directml-appropriate-for-my-project"></a><span data-ttu-id="5ba3a-113">DirectML 適用于我的專案嗎？</span><span class="sxs-lookup"><span data-stu-id="5ba3a-113">Is DirectML appropriate for my project?</span></span>

<span data-ttu-id="5ba3a-114">DirectML 是 [Windows Machine Learning](/windows/ai) 傘下的元件。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-114">DirectML is a component under the [Windows Machine Learning](/windows/ai) umbrella.</span></span> <span data-ttu-id="5ba3a-115">較高層級的 WinML API 主要是以模型為焦點，其具有負載系結-評估工作流程。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-115">The higher-level WinML API is primarily model-focused, with its load-bind-evaluate workflow.</span></span> <span data-ttu-id="5ba3a-116">但是像遊戲和引擎這樣的網域通常需要較低層級的抽象層級，而開發人員可以使用較高程度的控制項，才能充分利用晶片。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-116">But domains such as games and engines typically need a lower level of abstraction, and a higher degree of developer control, in order to take full advantage of the silicon.</span></span> <span data-ttu-id="5ba3a-117">如果您要計算毫秒數，並擠壓幀時間，則 DirectML 將符合您的機器學習需求。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-117">If you're counting milliseconds, and squeezing frame times, then DirectML will meet your machine learning needs.</span></span>

<span data-ttu-id="5ba3a-118">針對可靠的即時、高效能、低延遲及/或資源限制的案例，請使用 DirectML (而非 WinML) 。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-118">For reliable real-time, high-performance, low-latency, and/or resource-constrained scenarios, use DirectML (rather than WinML).</span></span> <span data-ttu-id="5ba3a-119">您可以將 DirectML 直接整合至現有的引擎或轉譯管線。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-119">You can integrate DirectML directly into your existing engine or rendering pipeline.</span></span> <span data-ttu-id="5ba3a-120">或者，在更高層級的自訂機器學習架構和中介軟體，DirectML 可以在 Windows 上提供高效能的後端。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-120">Or, at a higher level for custom machine learning frameworks and middleware, DirectML can provide a high-performance backend on Windows.</span></span>

<span data-ttu-id="5ba3a-121">WinML 本身會使用 DirectML 做為它的其中一個後端來執行。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-121">WinML is itself implemented using DirectML as one of its backends.</span></span>

## <a name="what-work-does-directml-do-and-what-work-must-i-do-as-the-developer"></a><span data-ttu-id="5ba3a-122">DirectML 執行的工作有哪些;開發 *人員必須做* 什麼工作？</span><span class="sxs-lookup"><span data-stu-id="5ba3a-122">What work does DirectML do; and what work must *I* do as the developer?</span></span>

<span data-ttu-id="5ba3a-123">DirectML 會在 GPU (或 AI 加速核心（如果有) ）上，有效率地執行您推斷模型的個別層級。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-123">DirectML efficiently executes the individual layers of your inference model on the GPU (or on AI-acceleration cores, if present).</span></span> <span data-ttu-id="5ba3a-124">每一層都是一個操作員，DirectML 可為您提供低層級、硬體加速機器學習基本運算子的程式庫。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-124">Each layer is an operator, and DirectML provides you with a library of low-level, hardware-accelerated machine learning primitive operators.</span></span> <span data-ttu-id="5ba3a-125">這些運算子有專為其設計的硬體特定和架構特定的優化 (詳細說明 [DirectML 執行此作業的部分為何？](#why-does-directml-perform-so-well)) 。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-125">These operators have hardware-specific and architecture-specific optimizations designed in to them (more on that in the section [Why does DirectML perform so well?](#why-does-directml-perform-so-well)).</span></span> <span data-ttu-id="5ba3a-126">同時，您的開發人員會看到一個與廠商無關的介面來執行這些運算子。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-126">At the same time, you as the developer see a single, vendor-agnostic interface for executing those operators.</span></span>

<span data-ttu-id="5ba3a-127">DirectML 中的運算子程式庫會提供您預期能夠在機器學習工作負載中使用的所有一般作業。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-127">The library of operators in DirectML supplies all of the usual operations that you'd expect to be able to use in a machine learning workload.</span></span>

- <span data-ttu-id="5ba3a-128">啟用運算子，例如 **線性**、 **ReLU**、 **sigmoid**、 **tanh** 等等。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-128">Activation operators, such as **linear**, **ReLU**, **sigmoid**, **tanh**, and more.</span></span>
- <span data-ttu-id="5ba3a-129">專案取向運算子，例如 **add**、 **exp**、 **log**、 **max**、 **min**、 **sub** 等等。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-129">Element-wise operators, such as **add**, **exp**, **log**, **max**, **min**, **sub**, and more.</span></span>
- <span data-ttu-id="5ba3a-130">卷積運算子，例如2D 和 3D **卷積** 等等。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-130">Convolution operators, such as 2D and 3D **convolution**, and more.</span></span>
- <span data-ttu-id="5ba3a-131">減少運算子，例如 **argmin**、 **average**、 **l2**、 **sum** 等等。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-131">Reduction operators, such as **argmin**, **average**, **l2**, **sum**, and more.</span></span>
- <span data-ttu-id="5ba3a-132">共用運算子，例如 **average**、 **lp** 和 **max**。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-132">Pooling operators, such as **average**, **lp**, and **max**.</span></span>
- <span data-ttu-id="5ba3a-133">類神經網路 (NN) 運算子，例如 **gemm**、 **gru**、 **lstm** 和 **rnn**。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-133">Neural network (NN) operators, such as **gemm**, **gru**, **lstm**, and **rnn**.</span></span>
- <span data-ttu-id="5ba3a-134">還有更多。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-134">And many more.</span></span>

<span data-ttu-id="5ba3a-135">為了達到最大效能，且您不需支付未使用的部分，DirectML 會將控制項放在您的手中，以開發人員瞭解如何在硬體上執行機器學習工作負載。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-135">For maximal performance, and so that you don't pay for what you don't use, DirectML puts the control into your hands as a developer over how your machine learning workload is executed on the hardware.</span></span> <span data-ttu-id="5ba3a-136">找出要執行的操作員，以及您身為開發人員的責任。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-136">Figuring out which operators to execute, and when, is your responsibility as the developer.</span></span> <span data-ttu-id="5ba3a-137">您可自行決定的工作包括：轉譯模型;簡化和優化您的層級;正在載入加權;資源配置、系結、記憶體管理 (就像使用 Direct3D 12) ;和圖形的執行。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-137">Tasks that are left to your discretion include: transcribing the model; simplifying and optimizing your layers; loading weights; resource allocation, binding, memory management (just as with Direct3D 12); and execution of the graph.</span></span>

<span data-ttu-id="5ba3a-138">您可以保留圖形的高階知識 (您可以直接對模型進行硬式編碼，也可以撰寫自己的模型載入器) 。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-138">You retain high-level knowledge of your graphs (you can hard-code your model directly, or you can write your own model loader).</span></span> <span data-ttu-id="5ba3a-139">您可能會設計消除倍增模型，例如，每個 **upsample**、**卷積** **、正規化和\*\*\*\*啟用** 運算子都使用數個層級。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-139">You might design an upscaling model, for example, using several layers each of **upsample**, **convolution**, **normalization**, and **activation** operators.</span></span> <span data-ttu-id="5ba3a-140">藉由熟悉、謹慎排程和屏障管理，您就可以從硬體中解壓縮最多的平行處理原則和效能。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-140">With that familiarity, careful scheduling, and barrier management, you can extract the most parallelism and performance from the hardware.</span></span> <span data-ttu-id="5ba3a-141">如果您正在開發遊戲，則您可以對排程進行謹慎的資源管理和控制，讓您能夠交錯機器學習工作負載和傳統的轉譯工作，以便讓 GPU 飽和。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-141">If you're developing a game, then your careful resource management and control over scheduling enables you to interleave machine learning workloads and traditional rendering work in order to saturate the GPU.</span></span>

## <a name="whats-the-high-level-directml-workflow"></a><span data-ttu-id="5ba3a-142">高層級 DirectML 工作流程是什麼？</span><span class="sxs-lookup"><span data-stu-id="5ba3a-142">What's the high-level DirectML workflow?</span></span>

<span data-ttu-id="5ba3a-143">以下是我們預期如何使用 DirectML 的高階配方。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-143">Here's the high-level recipe for how we expect DirectML to be used.</span></span> <span data-ttu-id="5ba3a-144">在初始化和執行的兩個主要階段中，您可以將工作記錄至命令清單，然後在佇列上執行它們。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-144">Within the two main phases of initialization and execution, you record work into command lists and then you execute them on a queue.</span></span>

### <a name="initialization"></a><span data-ttu-id="5ba3a-145">初始化</span><span class="sxs-lookup"><span data-stu-id="5ba3a-145">Initialization</span></span>

1. <span data-ttu-id="5ba3a-146">建立 direct3d 12 的資源 &mdash; ： direct3d 12 裝置、命令佇列、命令清單和資源（例如描述項堆積）。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-146">Create your Direct3D 12 resources&mdash;the Direct3D 12 device, command queue, command list, and resources such as descriptor heaps.</span></span>
2. <span data-ttu-id="5ba3a-147">因為您正在執行機器學習推斷以及轉譯工作負載，所以請建立 &mdash; DirectML 裝置和操作員實例的 DirectML 資源。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-147">Since you're doing machine learning inferencing as well as your rendering workload, create DirectML resources&mdash;the DirectML device, and operator instances.</span></span> <span data-ttu-id="5ba3a-148">如果您有機器學習模型，而您需要使用特定的資料類型，以特定的篩選 tensor 大小來執行特定類型的卷積，則這些都是 DirectML 的 **卷積** 運算子中的所有參數。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-148">If you have a machine learning model where you need to perform a particular type of convolution with a particular size of filter tensor with a particular data type, then those are all parameters into DirectML's **convolution** operator.</span></span>
3. <span data-ttu-id="5ba3a-149">DirectML 記錄可用於 Direct3D 12 命令清單。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-149">DirectML records work into Direct3D 12 command lists.</span></span> <span data-ttu-id="5ba3a-150">因此，完成初始化之後，您可以記錄 (的系結和初始化，例如，) 您的卷積運算子放入命令清單中。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-150">So, once initialization is done, you record the binding and initialization of (for example) your convolution operator into your command list.</span></span> <span data-ttu-id="5ba3a-151">然後，在佇列上關閉並執行您的命令清單（如同往常一樣）。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-151">Then, close and execute your command list on your queue as usual.</span></span>

### <a name="execution"></a><span data-ttu-id="5ba3a-152">執行</span><span class="sxs-lookup"><span data-stu-id="5ba3a-152">Execution</span></span>

1. <span data-ttu-id="5ba3a-153">將您的權數張量上傳至資源。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-153">Upload your weight tensors into resources.</span></span> <span data-ttu-id="5ba3a-154">DirectML 中的 tensor 會使用一般 Direct3D 12 資源來表示。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-154">A tensor in DirectML is represented using a regular Direct3D 12 resource.</span></span> <span data-ttu-id="5ba3a-155">例如，如果您想要將權數資料上傳至 GPU，那麼您可以使用與任何其他 Direct3D 12 資源相同的方式， (使用上傳堆積或複製佇列) 。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-155">For example, if you want to upload your weight data to the GPU, then you do that the same way you would with any other Direct3D 12 resource (use an upload heap, or the copy queue).</span></span>
2. <span data-ttu-id="5ba3a-156">接下來，您必須將這些 Direct3D 12 資源系結為您的輸入和輸出張量。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-156">Next, you need to bind those Direct3D 12 resources as your input and output tensors.</span></span> <span data-ttu-id="5ba3a-157">記錄到您的命令清單中，系結和操作員的執行。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-157">Record into your command list the binding and the execution of your operators.</span></span>
3. <span data-ttu-id="5ba3a-158">關閉並執行您的命令清單。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-158">Close and execute your command list.</span></span>

<span data-ttu-id="5ba3a-159">和 Direct3D 12 一樣，資源存留期和同步處理都是您的責任。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-159">Just as with Direct3D 12, resource lifetime and synchronization are your responsibility.</span></span> <span data-ttu-id="5ba3a-160">例如，請勿釋放您的 DirectML 物件，直到它們在 GPU 上完成執行為止。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-160">For example, don't release your DirectML objects at least until they've completed execution on the GPU.</span></span>

## <a name="why-does-directml-perform-so-well"></a><span data-ttu-id="5ba3a-161">為什麼 DirectML 執行得很好？</span><span class="sxs-lookup"><span data-stu-id="5ba3a-161">Why does DirectML perform so well?</span></span>

<span data-ttu-id="5ba3a-162">有一個很好的原因，就是您不應該只在 [計算著色器](/windows/desktop/direct3d12/pipelines-and-shaders-with-directx-12#direct3d-12-compute-pipeline)中撰寫自己的卷積運算子 (例如) 作為 HLSL。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-162">There's a good reason why you shouldn't just write your own convolution operator (for example) as HLSL in a [compute shader](/windows/desktop/direct3d12/pipelines-and-shaders-with-directx-12#direct3d-12-compute-pipeline).</span></span> <span data-ttu-id="5ba3a-163">使用 DirectML 的優點是， &mdash; 除了節省您自己 homebrewing 解決方案的優點之外， &mdash; 它還能提供您更好的效能，而不是使用手寫的一般用途計算著色器（如 **卷積** 或 **lstm**）來達成。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-163">The advantage of using DirectML is that&mdash;apart from saving you the effort of homebrewing your own solution&mdash;it has the capability of giving you much better performance than you could achieve with a hand-written, general-purpose compute shader for something like **convolution**, or **lstm**.</span></span>

<span data-ttu-id="5ba3a-164">由於 Direct3D 12 metacommands 功能的緣故，DirectML 可達成這個部分。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-164">DirectML achieves this in part due to the Direct3D 12 metacommands feature.</span></span> <span data-ttu-id="5ba3a-165">Metacommands 會公開一小 DirectML 的功能，可讓硬體廠商提供 DirectML 對廠商硬體特定和架構特定優化的存取。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-165">Metacommands expose a black box of functionality up to DirectML, which allows hardware vendors to provide DirectML access to vendor hardware-specific and architecture-specific optimizations.</span></span> <span data-ttu-id="5ba3a-166">例如，多個運算子 &mdash; （例如，加總的上積） &mdash; 可以 *合併* 成單一 metacommand。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-166">Multiple operators&mdash;for example, convolution followed by activation&mdash;can be *fused* together into a single metacommand.</span></span> <span data-ttu-id="5ba3a-167">基於這些因素，DirectML 能夠超越更妥善撰寫、且專為在各種硬體上執行而撰寫的手動調整計算著色器的效能。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-167">Because of these factors, DirectML has the capability to exceed the performance of even a very well-written hand-tuned compute shader written to run on a breadth of hardware.</span></span>

<span data-ttu-id="5ba3a-168">Metacommands 是 Direct3D 12 API 的一部分，不過它們是鬆散耦合的。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-168">Metacommands are part of the Direct3D 12 API, although they're loosely coupled to it.</span></span> <span data-ttu-id="5ba3a-169">Metacommand 是由固定的 [**GUID**](/windows/win32/api/guiddef/ns-guiddef-guid)所識別，而它的所有其他相關資訊 (從其行為和語義，到其簽章和名稱) 並不是 DIRECT3D 12 API 的一部分。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-169">A metacommand is identified by a fixed [**GUID**](/windows/win32/api/guiddef/ns-guiddef-guid), while almost everything else about it (from its behavior and semantics to its signature and name) are not strictly part of the Direct3D 12 API.</span></span> <span data-ttu-id="5ba3a-170">相反地，metacommand 是在其作者和執行它的驅動程式之間指定。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-170">Rather, a metacommand is specified between its author and the driver that implements it.</span></span> <span data-ttu-id="5ba3a-171">在此情況下，作者是 DirectML。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-171">In this case, the author is DirectML.</span></span> <span data-ttu-id="5ba3a-172">Metacommands 是 Direct3D 12 基本專案 (就像繪製和分派) 一樣，因此可以記錄到命令清單中並排程執行。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-172">Metacommands are Direct3D 12 primitives (just like Draws and Dispatches), so they can be recorded into a command list and scheduled for execution together.</span></span>

<span data-ttu-id="5ba3a-173">DirectML 使用整個機器學習 metacommands 套件來加速您的機器學習工作負載。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-173">DirectML accelerates your machine learning workloads using an entire suite of machine learning metacommands.</span></span> <span data-ttu-id="5ba3a-174">因此，您不需要撰寫廠商特定的程式碼路徑，就能為您的推斷達成硬體加速。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-174">Consequently, you don't need to write vendor-specific code paths to achieve hardware acceleration for your inferencing.</span></span> <span data-ttu-id="5ba3a-175">如果您在 AI 加速的晶片上執行，則 DirectML 會使用該硬體來大幅加速操作，例如卷積。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-175">If you happen to run on an AI-accelerated chip, then DirectML uses that hardware to greatly accelerate operations such as convolution.</span></span> <span data-ttu-id="5ba3a-176">您可以採用您所撰寫的相同程式碼，而不需要修改它，在不是 AI 加速的晶片上執行 (可能是您膝上型電腦中的整合式 GPU) ，而且仍會獲得絕佳的 GPU 硬體加速。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-176">You can take the same code that you wrote, without modifying it, run it on a chip that's not AI-accelerated (perhaps the integrated GPU in your laptop), and still get great GPU hardware acceleration.</span></span> <span data-ttu-id="5ba3a-177">如果沒有可用的 GPU，DirectML 就會回到 CPU。</span><span class="sxs-lookup"><span data-stu-id="5ba3a-177">And if no GPU is available, then DirectML falls back to the CPU.</span></span>
